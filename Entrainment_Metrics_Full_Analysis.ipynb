{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvOEQvU/oBDqzu6NzvO8EY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luzalbaposse/audio-process/blob/main/Entrainment_Metrics_Full_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrainment Metrics: analisis de conversaciones"
      ],
      "metadata": {
        "id": "cMRxAznLYhqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para usar este notebook, vas a necesitar seguir [estas instrucciones](https://docs.google.com/document/d/12kiauHlXamN-WvpGU1nBTOL9hAh1OJpdU6fqn3_3ITc/edit?usp=sharing) para preparar los archivos. Una vez que tengas eso, vamos a organizar esos archivos de audio en una carpeta `dyads` que contenga las conversaciones en carpetas llamadas `A{numero}`. Dentro de cada carpeta, vamos a necesitar tener nuestro `.TextGrid` y nuestros respectivos `.wav`. Además, antes de correr este notebook, vas a necesitar obtener los `.phrases `para esos wavs. Lo podes hacer utilizando este código:\n",
        "\n",
        "```\n",
        "#!/usr/bin/perl\n",
        "\n",
        "# Agustin Gravano - Columbia University - November 2005\n",
        "\n",
        "# Converts a Praat TextGrid file into a set of WaveSurfer annotation\n",
        "# files, one for each tier. Note: WaveSurfer makes no distinction between\n",
        "# point and interval tiers, so everything is stored as intervals.\n",
        "\n",
        "# Usage: praat2wavesurfer.pl STEM\n",
        "# That will read STEM.TextGrid and create STEM.words, STEM.tones, etc.\n",
        "\n",
        "$stem = shift;\n",
        "\n",
        "if ($stem =~ m/(.+)\\.TextGrid$/i) {\n",
        "\t$stem = $1;\n",
        "}\n",
        "\n",
        "if (!$stem) {\n",
        "    die \"Usage: praat2wavesurfer.pl STEM\\nThat will read STEM.TextGrid and create STEM.words, STEM.tones, etc.\\n\";  \n",
        "} elsif (! -e ($stem.\".TextGrid\")) {\n",
        "    die \"File not found: $stem.TextGrid\\n\";    \n",
        "}\n",
        "\n",
        "# read the source file into the @lines array.\n",
        "open SOURCEFILE, $stem.\".TextGrid\";\n",
        "@lines = <SOURCEFILE>;\n",
        "close SOURCEFILE;\n",
        "\n",
        "# remove the trailing \\n from each line.\n",
        "chomp @lines;\n",
        "\n",
        "# parse the header, upto this line: \"item []:\"\n",
        "\n",
        "$i = 0;\n",
        "my $starting_time = 0;\n",
        "my $finishing_time = 0;\n",
        "my $number_of_tiers = 0;\n",
        "\n",
        "$done = 0;\n",
        "\n",
        "while (!$done) {\n",
        "\tmy $line = $lines[$i];\n",
        "\n",
        "    if ($line =~ m/xmin =\\s*([0-9\\.]+)/) {\n",
        "    \t$starting_time = $1;\n",
        "    }\n",
        "    elsif ($line =~ m/xmax =\\s*([0-9\\.]+)/) {\n",
        "    \t$finishing_time = $1;\n",
        "    }\n",
        "    elsif ($line =~ m/size =\\s*([0-9]+)/) {\n",
        "    \t$number_of_tiers = $1;\n",
        "    }\n",
        "    elsif ($line =~ m/item \\[\\]:/) {\n",
        "    \t$done = 1;\n",
        "    }\n",
        "    elsif ($i>=@lines) {\n",
        "    \tdie \"Wrong format in $stem.TextGrid: Reached the end of file while expecting \\\"item []:\\\"\\n\";\n",
        "    }\n",
        "    $i++;\n",
        "}\n",
        "\n",
        "for ($tier_number = 1;  $tier_number <= $number_of_tiers;  $tier_number++) {\n",
        "\tmy $target_file_text = \"\";\n",
        "\n",
        "\tmy $done = 0;\n",
        "\tmy $tier_name = \"\";\n",
        "\tmy $tier_starting_time = 0;\n",
        "\tmy $tier_finishing_time = 0;\n",
        "\tmy $tier_number_of_points = 0;\n",
        "\tmy $tier_number_of_intervals = 0;\n",
        "\n",
        "\twhile (!$done) {\n",
        "\t\tmy $line = $lines[$i];\n",
        "\n",
        "\t    if ($line =~ m/name =\\s*\"([^\"]+)\"/) {\n",
        "\t    \t$tier_name = $1;\n",
        "\t    }\n",
        "\t    elsif ($line =~ m/xmin =\\s*([0-9\\.]+)/) {\n",
        "\t    \t$tier_starting_time = $1;\n",
        "\t    }\n",
        "\t    elsif ($line =~ m/xmax =\\s*([0-9\\.]+)/) {\n",
        "\t    \t$tier_finishing_time = $1;\n",
        "\t    }\n",
        "\t    elsif ($line =~ m/points: size =\\s*([0-9]+)/) {\n",
        "\t    \t$tier_number_of_points = $1;\n",
        "\t    \t$done = 1;\n",
        "\t    }\n",
        "\t    elsif ($line =~ m/intervals: size =\\s*([0-9]+)/) {\n",
        "\t    \t$tier_number_of_intervals = $1;\n",
        "\t    \t$done = 1;\n",
        "\t    }\n",
        "\t    elsif ($i>=@lines) {\n",
        "\t    \tdie \"Wrong format in $stem.TextGrid: Reached the end of file while expecting \\\"points: size = INTEGER\\\" or \\\"intervals: size = INTEGER\\\"\\n\";\n",
        "\t    }\n",
        "\t    \n",
        "\t    $i++;\n",
        "\t}\n",
        "\n",
        "\tif ($tier_number_of_points) {\n",
        "\t\tmy $previous_time = 0;\n",
        "\t\tfor ($j=1; $j<=$tier_number_of_points; $j++) {\n",
        "\t\t\tmy $done = 0;\n",
        "\t\t\tmy $time = 0;\n",
        "\t\t\tmy $mark = \"\";\n",
        "\n",
        "\t\t\tif (! $lines[$i] =~ \"points \\[$j\\]:\") {\n",
        "\t\t\t\tdie \"Wrong format in $stem.TextGrid: Expected: \\\"points [$j]:\\\" at line $i.\\n\";\n",
        "\t\t\t}\n",
        "\t\t\t$i++;\n",
        "\n",
        "\t\t\tif ($lines[$i] =~ m/number =\\s*([0-9\\.]+)/) {\n",
        "\t\t\t\t$time = $1;\n",
        "\t\t\t} else {\n",
        "\t\t\t\tdie \"Wrong format in $stem.TextGrid: Expected: \\\"number = FLOAT\\\" at line $i.\\n\";\n",
        "\t\t\t}\n",
        "\t\t\t$i++;\n",
        "\n",
        "\t\t\tif ($lines[$i] =~ m/mark =\\s*\"(.*)\" $/) {\n",
        "\t\t\t\t$mark = $1;\n",
        "\t\t\t} else {\n",
        "\t\t\t\tdie \"Wrong format in $stem.TextGrid: Expected: \\\"mark = STRING\\\" at line $i.\\n\";\n",
        "\t\t\t}\n",
        "\t\t\t$i++;\n",
        "\n",
        "\t\t\t# format the times\n",
        "\t\t\t$previous_time = f($previous_time);\n",
        "\t\t\t$time = f($time);\n",
        "\n",
        "\t\t\t$target_file_text .= \"$previous_time $time $mark\\n\";\n",
        "\t\t\t$previous_time = $time;\n",
        "\t\t}\n",
        "\t}\n",
        "\n",
        "\telsif ($tier_number_of_intervals) {\n",
        "\t\tfor ($j=1; $j<=$tier_number_of_intervals; $j++) {\n",
        "\t\t\tmy $done = 0;\n",
        "\t\t\tmy $xmin = 0;\n",
        "\t\t\tmy $xmax = 0;\n",
        "\t\t\tmy $text = \"\";\n",
        "\n",
        "\t\t\tif (! $lines[$i] =~ \"intervals \\[$j\\]:\") {\n",
        "\t\t\t\tdie \"Wrong format in $stem.TextGrid: Expected: \\\"intervals [$j]:\\\" at line $i.\\n\";\n",
        "\t\t\t}\n",
        "\t\t\t$i++;\n",
        "\n",
        "\t\t\tif ($lines[$i] =~ m/xmin =\\s*([0-9\\.]+)/) {\n",
        "\t\t\t\t$xmin = $1;\n",
        "\t\t\t} else {\n",
        "\t\t\t\tdie \"Wrong format in $stem.TextGrid: Expected: \\\"xmin = FLOAT\\\" at line $i.\\n\";\n",
        "\t\t\t}\n",
        "\t\t\t$i++;\n",
        "\n",
        "\t\t\tif ($lines[$i] =~ m/xmax =\\s*([0-9\\.]+)/) {\n",
        "\t\t\t\t$xmax = $1;\n",
        "\t\t\t} else {\n",
        "\t\t\t\tdie \"Wrong format in $stem.TextGrid: Expected: \\\"xmax = FLOAT\\\" at line $i.\\n\";\n",
        "\t\t\t}\n",
        "\t\t\t$i++;\n",
        "\n",
        "\t\t\tif ($lines[$i] =~ m/text =\\s*\"(.*)\" ?$/) {\n",
        "\t\t\t\t$text = $1;\n",
        "\t\t\t} else {\n",
        "\t\t\t\tdie \"Wrong format in $stem.TextGrid: Expected: \\\"text = STRING\\\" at line $i.\\n\";\n",
        "\t\t\t}\n",
        "\t\t\t$i++;\n",
        "\n",
        "\t\t\t# format the times\n",
        "\t\t\t$xmin = f($xmin);\n",
        "\t\t\t$xmax = f($xmax);\n",
        "\n",
        "\t\t\t$target_file_text .= \"$xmin $xmax $text\\n\";\n",
        "\t\t}\n",
        "\t}\n",
        "\n",
        "\topen TARGETFILE, \">\".$stem.\".\".$tier_name;\n",
        "\tprint TARGETFILE $target_file_text;\n",
        "\tclose TARGETFILE;\n",
        "}\n",
        "\n",
        "# number format\n",
        "sub f {\n",
        "\tmy $x = shift;\n",
        "\treturn sprintf(\"%.6f\", $x);\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "tYoRLNi9mEIW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OqnPMQ1iYAGX"
      },
      "outputs": [],
      "source": [
        "#@markdown ## Instalar dependencias\n",
        "!pip install fastapi\n",
        "!pip install kaleido\n",
        "!pip install python-multipart\n",
        "!pip install uvicorn\n",
        "!pip install entrainment_metrics\n",
        "!pip install pyannote"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## Importar dependencias\n",
        "from typing import List\n",
        "from entrainment_metrics import InterPausalUnit\n",
        "from pathlib import Path\n",
        "import glob\n",
        "import os\n",
        "import csv\n",
        "from scipy.io import wavfile\n",
        "from math import isnan\n",
        "from entrainment_metrics.continuous import plot_time_series\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import re\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "INZtjRhjZMWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ##Montar Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "bT1bnmvDZk6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dividir IPUS"
      ],
      "metadata": {
        "id": "Awh1fBj5Y8P0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversations = [path.split(\"/\")[1] for path in glob.glob(os.path.join(Path(\"/content/drive/MyDrive/dyads/*\")))]"
      ],
      "metadata": {
        "id": "OlEZHv_rZdsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(conversations)"
      ],
      "metadata": {
        "id": "KgqJEz_sZqiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for conversation in conversations:\n",
        "    print(conversation)\n",
        "    conversation_path = Path(f\"/content/drive/MyDrive/dyads/A{conversation}\")\n",
        "    phrases = [phrases_file for phrases_file in conversation_path.glob('*.phrases')]\n",
        "    if phrases:\n",
        "        print(\"Phrases already calculated\")\n",
        "        continue\n",
        "    task_fn = f\"/content/drive/MyDrive/dyads/A{conversation}/*.TextGrid\"\n",
        "    with open(task_fn, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "        debate_start, debate_end = None, None\n",
        "        for i in range(len(lines)):\n",
        "            if \"debate\" in lines[i]:\n",
        "                debate_start = float(lines[i-2].split(\"xmin = \")[1])\n",
        "                debate_end = float(lines[i-1].split(\"xmax = \")[1])\n",
        "\n",
        "        if debate_start is None or debate_end is None:\n",
        "            raise ValueError(f\"Could not find debate line in {task_fn}\")\n",
        "    #print(f\"start {debate_start}\")\n",
        "    #print(f\"end {debate_end}\")\n",
        "    wavs_fnames = conversation_path.glob('*.wav')\n",
        "    for wav_file in wavs_fnames:\n",
        "        if \"_cropped\" in wav_file.stem:\n",
        "            continue\n",
        "        samplerate, data = wavfile.read(wav_file)\n",
        "\n",
        "        start, end = int(debate_start * samplerate), int(debate_end * samplerate)\n",
        "        cutted_data = data[start : end]\n",
        "\n",
        "        cropped_wav_name: str = wav_file.stem + \"_cropped.wav\"\n",
        "        output_dir: str = os.path.join(conversation_path, cropped_wav_name)\n",
        "        wavfile.write(output_dir, samplerate, cutted_data)"
      ],
      "metadata": {
        "id": "D_rnreODZu-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Obtener IPUS de los Wavs usando Voice Activity Detection"
      ],
      "metadata": {
        "id": "liT1_aXXZ0mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyannote.audio import Pipeline\n",
        "from entrainment_metrics import InterPausalUnit"
      ],
      "metadata": {
        "id": "lbV_JUobZw-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline.from_pretrained(\"pyannote/voice-activity-detection\",\n",
        "                                    use_auth_token=\"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\")"
      ],
      "metadata": {
        "id": "W9srJYnlaFx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ipus = {}"
      ],
      "metadata": {
        "id": "WZJzHIOUaHso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for conversation in conversations:\n",
        "    print(conversation)\n",
        "    conversation_path = Path(f\"./dyads/{conversation}\")\n",
        "    phrases = [phrases_file for phrases_file in conversation_path.glob('*.phrases')]\n",
        "    if phrases:\n",
        "        print(\"Phrases already calculated\")\n",
        "        continue\n",
        "    wavs_fnames = Path(conversation_path).glob('*_cropped.wav')\n",
        "    if not conversation in ipus:\n",
        "        ipus[conversation] = {}\n",
        "    for wav_file in wavs_fnames:\n",
        "        speaker = wav_file.stem.lower().replace(\"participante\", \"\").replace(\"_cropped\", \"\").replace(\" \", \"\").replace(\"audio\", \"\")[0].upper()\n",
        "        print(wav_file)\n",
        "        print(speaker)\n",
        "        if not speaker in ipus[conversation]:\n",
        "            print(f\"Processing {speaker}\")\n",
        "            try:\n",
        "                ipus[conversation][speaker] = []\n",
        "                output = pipeline(wav_file)\n",
        "                for speech in output.get_timeline().support():\n",
        "                    ipu = InterPausalUnit(start=speech.start, end=speech.end)\n",
        "                    ipus[conversation][speaker].append(ipu)\n",
        "            except:\n",
        "                print(f\"Failed {conversation}.{speaker}\")"
      ],
      "metadata": {
        "id": "VhuyniA0aJZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversations_to_fix = []\n",
        "for conversation, conversation_dict in ipus.items():\n",
        "    if len(conversation_dict) != 2:\n",
        "        conversations_to_fix.append(conversation)\n",
        "    if \"A\" not in conversation_dict or \"Z\" not in conversation_dict:\n",
        "        conversations_to_fix.append(conversation)\n",
        "if conversations_to_fix:\n",
        "    print(f\"FIX: {conversations_to_fix}\")"
      ],
      "metadata": {
        "id": "neSPkIIlaORA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for conversation, speakers_dict in ipus.items():\n",
        "    if len(speakers_dict) != 2:\n",
        "        raise ValueError(f\"Wrong amount of speakers {conversation}: {list(ipus[conversation].keys())}\")"
      ],
      "metadata": {
        "id": "dCEOpeVCaPKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crear .phrases para cada speaker"
      ],
      "metadata": {
        "id": "WMCiTwFCi3be"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_phrases_file(conversation_id, speaker_id, ipus_list):\n",
        "    # TODO Watchout when len(ipus_list) <=3\n",
        "    lines = []\n",
        "    if ipus_list[0].start > 0:\n",
        "        lines.append(f\"0.0 {ipus_list[0].start} #\")\n",
        "    for i in range(len(ipus_list) -1) :\n",
        "        lines.append(f\"{ipus_list[i].start} {ipus_list[i].end} IPU\")\n",
        "        lines.append(f\"{ipus_list[i].end} {ipus_list[i+1].start} #\")\n",
        "\n",
        "    lines.append(f\"{ipus_list[-1].start} {ipus_list[-1].end} IPU\")\n",
        "\n",
        "    with open(f'/content/drive/MyDrive/dyads/{conversation_id}/{speaker_id}.phrases', 'w') as f:\n",
        "        f.writelines(\"%s\\n\" % l for l in lines)"
      ],
      "metadata": {
        "id": "x6I94Xt6i6nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for conversation in ipus:\n",
        "    conversation_ipus = ipus[conversation]\n",
        "    for speaker, speaker_ipus in conversation_ipus.items():\n",
        "        # Ruta del directorio de la conversación\n",
        "        conversation_path = \"/content/drive/MyDrive/dyads/A44\"\n",
        "\n",
        "        # Obtener lista de archivos en el directorio\n",
        "        files_in_directory = os.listdir(conversation_path)\n",
        "\n",
        "        # Filtrar archivos que terminan con \"_mono.phrases\"\n",
        "        mono_phrases_files = [file for file in files_in_directory if file.endswith(f\"{conversation}.phrases\")]\n",
        "\n",
        "        # Renombrar archivos\n",
        "        for mono_phrases_file in mono_phrases_files:\n",
        "            new_file_name = f\"{conversation}_.phrases\"\n",
        "            os.rename(os.path.join(conversation_path, mono_phrases_file), os.path.join(conversation_path, new_file_name))\n"
      ],
      "metadata": {
        "id": "-fZFDRwQi7tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for conversation in ipus:\n",
        "    conversation_ipus = ipus[conversation]\n",
        "    for speaker, speaker_ipus in conversation_ipus.items():\n",
        "        write_phrases_file(conversation, speaker, speaker_ipus)"
      ],
      "metadata": {
        "id": "T_pelo5ai_UT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conseguir InterPausalUnits de los .phrases"
      ],
      "metadata": {
        "id": "P6aCAkjSjVcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_interpausal_units_from_phrases(phrases_fname: Path) -> List[InterPausalUnit]:\n",
        "    \"\"\"\n",
        "    Return a list of IPUs given a Path to a .phrases file\n",
        "\n",
        "    The format of the file must be:\n",
        "        - For each line\n",
        "            f'{starting_time} {ending_time} {ipu's transcription}'\n",
        "        Where starting_time and ending_time are floats\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    phrases_fname: Path\n",
        "        The path to the words file\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List[InterPausalUnit]\n",
        "        The InterPausalUnits from the words file.\n",
        "    \"\"\"\n",
        "    interpausal_units: List[InterPausalUnit] = []\n",
        "    skip_words = [\"<risa>\", \"<tos>\", \"#\", \"<chasquido>\", \"<ruido>\", \"<risas>\", \"<missing>\", \"<chasquidos>\", \"<suspiro>\", \"<silbando?>\" , \"<silbando?>\"]\n",
        "    with open(phrases_fname, \"r\") as word_file:\n",
        "        while line := word_file.readline().rstrip():\n",
        "            line_splitted = line.split()\n",
        "            start, end, first_word = line_splitted[0], line_splitted[1], line_splitted[2]\n",
        "\n",
        "            if (first_word in skip_words and len(line_splitted) == 3) or ((len(line_splitted) == 4 and first_word in skip_words and line_splitted[3] in skip_words)):\n",
        "                continue\n",
        "            else:\n",
        "                interpausal_units.append(InterPausalUnit(float(start), float(end)))\n",
        "    return interpausal_units"
      ],
      "metadata": {
        "id": "Vg_9ZmMWjepj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_interpausal_units_from_phrases(phrases_fname: Path) -> List[InterPausalUnit]:\n",
        "    interpausal_units: List[InterPausalUnit] = []\n",
        "    skip_words = [\"<risa>\", \"<tos>\", \"#\", \"<chasquido>\", \"<ruido>\", \"<risas>\", \"<missing>\", \"<chasquidos>\", \"<suspiro>\", \"<silbando?>\", \"<silbando?>\"]\n",
        "    with open(phrases_fname, \"r\") as word_file:\n",
        "        for line in word_file:\n",
        "            line = line.rstrip()\n",
        "            line_splitted = line.split()\n",
        "\n",
        "            # Asegurarse de que la línea tenga suficientes elementos\n",
        "            if len(line_splitted) < 3:\n",
        "                print(f\"Línea no válida (menos de 3 elementos): {line}\")\n",
        "                continue\n",
        "\n",
        "            start, end, first_word = line_splitted[0], line_splitted[1], line_splitted[2]\n",
        "\n",
        "            if (first_word in skip_words and len(line_splitted) == 3) or ((len(line_splitted) == 4 and first_word in skip_words and line_splitted[3] in skip_words)):\n",
        "                continue\n",
        "            else:\n",
        "                interpausal_units.append(InterPausalUnit(float(start), float(end)))\n",
        "    return interpausal_units\n"
      ],
      "metadata": {
        "id": "uTHgdtALjgWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ipus = {}"
      ],
      "metadata": {
        "id": "Fpl0mxfmjhX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"/content/drive/MyDrive/dyads\"\n",
        "\n",
        "# Itera sobre las carpetas A{numero} en la carpeta base\n",
        "for number in range(1, 53):\n",
        "    folder_name = f\"A{number}\"\n",
        "    folder_path = os.path.join(base_path, folder_name)\n",
        "\n",
        "    # Verifica si la carpeta existe\n",
        "    if os.path.exists(folder_path):\n",
        "        print(f\"\\nEstoy en la carpeta '{folder_name}':\")\n",
        "\n",
        "        # Itera sobre los archivos *.phrases en la carpeta específica\n",
        "        for filename in glob.glob(os.path.join(Path(folder_path), \"*.phrases\")):\n",
        "            speaker = filename.split(\"/\")[-1].replace(\".phrases\", \"\")\n",
        "            print(speaker)\n",
        "            if not number in ipus:\n",
        "                ipus[number] = {}\n",
        "            # interpausal units usando \"get_interpausal_units_from_phrases\"\n",
        "            # try:\n",
        "            ipus[number][speaker] = get_interpausal_units_from_phrases(filename)\n",
        "            # except:\n",
        "            #     raise ValueError(f\"{session}.{task}.{speaker_id}\")\n",
        "\n",
        "print(\"\\nListo :D\")"
      ],
      "metadata": {
        "id": "ynkrnfB9jj7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for silence between IPUs < 100ms\n",
        "for conversation, speaker_dict in ipus.items():\n",
        "    for speaker, speaker_ipus in speaker_dict.items():\n",
        "        for i in range(len(speaker_ipus) - 1):\n",
        "            if speaker_ipus[i + 1].start - speaker_ipus[i].end < 0.1:\n",
        "                raise ValueError(f\"Found diff < 100ms: {conversation}.{speaker} {speaker_ipus[i]} <-> {speaker_ipus[i + 1]} \")"
      ],
      "metadata": {
        "id": "eVBV4MGSjmz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calcular Feature Values"
      ],
      "metadata": {
        "id": "PWyROf2ujpph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para esta sección, vamos a necesitar los géneros de las personas que hayan participado en las conversaciones. Este `Genders.csv` debería tener 3 columnas: `id`, siendo este el código de participante, `pareja`, que es el código de la conversación, y `gender` que es el género de la persona en cuestión."
      ],
      "metadata": {
        "id": "sxfHwiBvjs4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_genders = pd.read_csv(\"/content/Genders.csv\")\n",
        "genders = df_genders.set_index(['id', 'pareja']).T.to_dict('dict')"
      ],
      "metadata": {
        "id": "--bb1pcAjr7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_ipus_features(ipus_list, wav_fname, gender):\n",
        "'''\n",
        "\n",
        "'''\n",
        "    for ipu in ipus_list:\n",
        "        try:\n",
        "            ipu.calculate_features(\n",
        "                audio_file=wav_fname,\n",
        "                extractor=\"speech-rate\",\n",
        "            )\n",
        "        except ValueError as err:\n",
        "            ipu_too_small = 'is too small to resample from'\n",
        "\n",
        "            if ipu_too_small in str(err):\n",
        "                pass\n",
        "            else:\n",
        "                raise err\n",
        "        try:\n",
        "            ipu.calculate_features(\n",
        "                audio_file=wav_fname,\n",
        "                extractor=\"praat\",\n",
        "                pitch_gender=gender,\n",
        "            )\n",
        "        except BaseException as err:\n",
        "            ipu_too_small = 'Audio file contains 0 samples'\n",
        "\n",
        "            if ipu_too_small in str(err):\n",
        "                pass\n",
        "            else:\n",
        "                raise err"
      ],
      "metadata": {
        "id": "qM5yOkfNkLCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for conversation, speaker_dict in ipus.items():\n",
        "    if len(speaker_dict) != 2:\n",
        "        print(f\"Unexpected number of speakers ({len(speaker_dict)}) in conversation A{conversation}. Skipping.\")\n",
        "        continue\n",
        "    conversation_wavs = glob.glob(os.path.join(Path(f\"/content/drive/MyDrive/dyads/A{conversation}\"), \"*_cropped.wav\"))\n",
        "    for speaker, speaker_ipus in speaker_dict.items():\n",
        "        print(f\"A{conversation}.{speaker}\")\n",
        "        key = (int(''.join(filter(str.isdigit, speaker[:4]))), f\"A{conversation}\")\n",
        "        print(\"Key:\", key)\n",
        "        if speaker_ipus:  # Verifica si la lista no está vacía\n",
        "            ipus_already_calculated = bool(speaker_ipus[0].features_values)\n",
        "            if ipus_already_calculated:\n",
        "                continue\n",
        "        if key in genders:\n",
        "            speaker_gender = genders[key]['gender']\n",
        "            print(\"Gender:\", speaker_gender)\n",
        "            wav_files = [wavfile for wavfile in conversation_wavs if speaker.lower()[:3] == os.path.basename(wavfile).lower()[:3] and wavfile.endswith(\"_cropped.wav\")]\n",
        "\n",
        "            for wav_file in wav_files:\n",
        "                extract_ipus_features(speaker_ipus, wav_file, speaker_gender)\n",
        "        else:\n",
        "            print(f\"Key {key} not found in genders\")"
      ],
      "metadata": {
        "id": "gQjKHcwakOed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Guardar en un CSV"
      ],
      "metadata": {
        "id": "IXQU9WMYkehq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = [\n",
        " 'SECONDS',\n",
        " 'F0_MAX',\n",
        " 'F0_MIN',\n",
        " 'F0_MEAN',\n",
        " 'F0_MEDIAN',\n",
        " 'F0_STDV',\n",
        " 'F0_MAS',\n",
        " 'ENG_MAX',\n",
        " 'ENG_MIN',\n",
        " 'ENG_MEAN',\n",
        " 'ENG_STDV',\n",
        " 'VCD2TOT_FRAMES',\n",
        " 'speech_rate',\n",
        "]"
      ],
      "metadata": {
        "id": "BnfmJuqRki8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "for conversation, speaker_dict in ipus.items():\n",
        "    for speaker, speaker_ipus in speaker_dict.items():\n",
        "            for ipu in speaker_ipus:\n",
        "\n",
        "                row = [conversation, speaker, ipu.start, ipu.end]\n",
        "\n",
        "                for feature in features:\n",
        "                    try:\n",
        "                        ipu_feature = ipu.feature_value(feature)\n",
        "                    except ValueError:\n",
        "                        ipu_feature = None\n",
        "                    row.append(ipu_feature)\n",
        "                rows.append(row)\n",
        "print(len(rows))"
      ],
      "metadata": {
        "id": "OfqZLQEnkjVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('ipus.csv', 'w', newline ='') as csvfile:\n",
        "    header = [\"conversation\", \"speaker\", \"start\", \"end\"]\n",
        "    header += features\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerows([header] + rows)"
      ],
      "metadata": {
        "id": "IhlX22YdkkKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calcular Entrainment Metrics"
      ],
      "metadata": {
        "id": "Cq9MJzk2kmk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importar el archivo ipus.csv"
      ],
      "metadata": {
        "id": "cU9oVo6nkuJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from entrainment_metrics.continuous import calculate_metric, TimeSeries"
      ],
      "metadata": {
        "id": "z7kV0Eshko-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ipus = pd.read_csv(\"./ipus.csv\")\n",
        "display(df_ipus)"
      ],
      "metadata": {
        "id": "53DQ8Ut8kqK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtener los IPUs"
      ],
      "metadata": {
        "id": "pelAmEnjkxdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Getting IPUs from DataFrame\n",
        "\"\"\"\n",
        "def get_ipus_from_conversation(df_ipus, conversation_id):\n",
        "    # Filtrar solo la conversación deseada\n",
        "    df_conversation = df_ipus[df_ipus[\"conversation\"] == conversation_id]\n",
        "\n",
        "    # Asumiendo que solo hay dos speakers por conversación,\n",
        "    # obtener sus identificadores únicos\n",
        "    speakers = df_conversation[\"speaker\"].unique()\n",
        "\n",
        "    if len(speakers) != 2:\n",
        "        # Manejar el caso donde no hay exactamente dos speakers\n",
        "        print(f\"Unexpected number of speakers ({len(speakers)}) found in conversation {conversation_id}\")\n",
        "\n",
        "    # Separar los IPUs para cada speaker\n",
        "    df_speaker_1 = df_conversation[df_conversation[\"speaker\"] == speakers[0]]\n",
        "    df_speaker_2 = df_conversation[df_conversation[\"speaker\"] == speakers[1]]\n",
        "\n",
        "    ipus_speaker_1 = get_ipus_from_speaker(df_speaker_1)\n",
        "    ipus_speaker_2 = get_ipus_from_speaker(df_speaker_2)\n",
        "\n",
        "    return ipus_speaker_1, ipus_speaker_2\n",
        "\n",
        "def get_ipus_from_speaker(df_speaker):\n",
        "    features = [\n",
        "     'F0_MAX', 'F0_MIN', 'F0_MEAN', 'F0_MEDIAN', 'F0_STDV', 'F0_MAS',\n",
        "     'ENG_MAX', 'ENG_MIN', 'ENG_MEAN', 'ENG_STDV', 'VCD2TOT_FRAMES', 'speech_rate',\n",
        "    ]\n",
        "    ipus = []\n",
        "    for _, row in df_speaker.iterrows():\n",
        "        ipu_feature_values = {feature: row[feature] for feature in features}\n",
        "        try:\n",
        "            # Extraer la parte relevante de la cadena y convertirla en float\n",
        "            start = float(row[\"start\"].split('.')[0] + '.' + row[\"start\"].split('.')[1][:3])\n",
        "            end = float(row[\"end\"].split('.')[0] + '.' + row[\"end\"].split('.')[1][:3])\n",
        "        except ValueError:\n",
        "\n",
        "            print(f\"Error converting start/end to float: start={row['start']}, end={row['end']}\")\n",
        "            continue\n",
        "\n",
        "        ipu = InterPausalUnit(\n",
        "            start=start,\n",
        "            end=end,\n",
        "            features_values=ipu_feature_values\n",
        "        )\n",
        "        ipus.append(ipu)\n",
        "    return ipus"
      ],
      "metadata": {
        "id": "eiA33tALkzZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculr las métricas"
      ],
      "metadata": {
        "id": "kP8Fgu2Yk1Zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metrics_from_conversation(df_ipus, conversation_id):\n",
        "    features = [\n",
        "     'F0_MAX',\n",
        "     'F0_MIN',\n",
        "     'F0_MEAN',\n",
        "     'F0_MEDIAN',\n",
        "     'F0_STDV',\n",
        "     'F0_MAS',\n",
        "     'ENG_MAX',\n",
        "     'ENG_MIN',\n",
        "     'ENG_MEAN',\n",
        "     'ENG_STDV',\n",
        "     'VCD2TOT_FRAMES',\n",
        "     'speech_rate',\n",
        "    ]\n",
        "    metrics = [\"synchrony\", \"convergence\", \"proximity\"]\n",
        "\n",
        "    ipus_a, ipus_b = get_ipus_from_conversation(df_ipus, conversation_id)\n",
        "    results = {}\n",
        "    for feature in features:\n",
        "        try:\n",
        "            ipus_a_w_feature = [ipu for ipu in ipus_a if ipu.feature_value(feature) is not None and not isnan(ipu.feature_value(feature))]\n",
        "            ipus_b_w_feature = [ipu for ipu in ipus_b if ipu.feature_value(feature) is not None and not isnan(ipu.feature_value(feature))]\n",
        "            if not ipus_a_w_feature or not ipus_b_w_feature:\n",
        "                for metric in metrics:\n",
        "                    results[feature + \"_\" + metric] = None\n",
        "                continue\n",
        "            time_series_a = TimeSeries(\n",
        "                feature=feature,\n",
        "                interpausal_units=ipus_a_w_feature,\n",
        "                method=\"knn\",\n",
        "            )\n",
        "            time_series_b = TimeSeries(\n",
        "                feature=feature,\n",
        "                interpausal_units=ipus_b_w_feature,\n",
        "                method=\"knn\",\n",
        "            )\n",
        "            if feature == \"speech_rate\":\n",
        "                plot_time_series(time_series_a, time_series_b)\n",
        "            for metric in metrics:\n",
        "                res_metric = calculate_metric(\n",
        "                    metric=metric,\n",
        "                    time_series_a=time_series_a,\n",
        "                    time_series_b=time_series_b,\n",
        "                )\n",
        "                results[feature + \"_\" + metric] = res_metric\n",
        "        except ValueError as err:\n",
        "            k_too_big = 'k cannot be bigger than the amount of interpausal units, default k is 7'\n",
        "\n",
        "            if k_too_big in str(err):\n",
        "                for metric in metrics:\n",
        "                    results[feature + \"_\" + metric] = None\n",
        "                print(k_too_big)\n",
        "            else:\n",
        "                print([ipu.features_values[feature] for ipu in ipus_a_w_feature])\n",
        "                print([ipu.features_values[feature] for ipu in ipus_b_w_feature])\n",
        "                raise err\n",
        "    return results\n",
        "\n",
        "def get_metrics_for_all_conversations(df_ipus):\n",
        "    res = {}\n",
        "    conversations = set(df_ipus[\"conversation\"].tolist())\n",
        "    for conversation in conversations:\n",
        "        res[conversation] = get_metrics_from_conversation(df_ipus, conversation)\n",
        "    return res"
      ],
      "metadata": {
        "id": "o2LASdQPk3D8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = get_metrics_for_all_conversations(df_ipus)"
      ],
      "metadata": {
        "id": "WDHlsGeWk5iM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Guardar en un csv"
      ],
      "metadata": {
        "id": "1Jvo0Cu6k7CF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_names = [\n",
        " 'F0_MAX_synchrony',\n",
        " 'F0_MAX_convergence',\n",
        " 'F0_MAX_proximity',\n",
        " 'F0_MIN_synchrony',\n",
        " 'F0_MIN_convergence',\n",
        " 'F0_MIN_proximity',\n",
        " 'F0_MEAN_synchrony',\n",
        " 'F0_MEAN_convergence',\n",
        " 'F0_MEAN_proximity',\n",
        " 'F0_MEDIAN_synchrony',\n",
        " 'F0_MEDIAN_convergence',\n",
        " 'F0_MEDIAN_proximity',\n",
        " 'F0_STDV_synchrony',\n",
        " 'F0_STDV_convergence',\n",
        " 'F0_STDV_proximity',\n",
        " 'F0_MAS_synchrony',\n",
        " 'F0_MAS_convergence',\n",
        " 'F0_MAS_proximity',\n",
        " 'ENG_MAX_synchrony',\n",
        " 'ENG_MAX_convergence',\n",
        " 'ENG_MAX_proximity',\n",
        " 'ENG_MIN_synchrony',\n",
        " 'ENG_MIN_convergence',\n",
        " 'ENG_MIN_proximity',\n",
        " 'ENG_MEAN_synchrony',\n",
        " 'ENG_MEAN_convergence',\n",
        " 'ENG_MEAN_proximity',\n",
        " 'ENG_STDV_synchrony',\n",
        " 'ENG_STDV_convergence',\n",
        " 'ENG_STDV_proximity',\n",
        " 'VCD2TOT_FRAMES_synchrony',\n",
        " 'VCD2TOT_FRAMES_convergence',\n",
        " 'VCD2TOT_FRAMES_proximity',\n",
        " 'speech_rate_synchrony',\n",
        " 'speech_rate_convergence',\n",
        " 'speech_rate_proximity'\n",
        "]"
      ],
      "metadata": {
        "id": "dg86NoV8k-HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "for conversation, conversation_metrics in results.items():\n",
        "    row = [conversation]\n",
        "    row += list(conversation_metrics.values())\n",
        "\n",
        "    rows.append(row)"
      ],
      "metadata": {
        "id": "tHUTPAlrk_3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_metrics = pd.read_csv(\"./metrics.csv\")\n",
        "display(df_metrics)"
      ],
      "metadata": {
        "id": "8ILDUGOvlBQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Troubleshotting"
      ],
      "metadata": {
        "id": "bGxL7c0dmT7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al hacer el análisis me encontré con varios errores y problemas. Dejo acá anotados la mayoría y la forma de que lo solucioné."
      ],
      "metadata": {
        "id": "brMSfVYnmWfN"
      }
    }
  ]
}